---
title: "Hemiptera metabarcoding analysis"
author: "Alexander Piper"
date: "2018/12/11"
output:
  html_document: default
  pdf_document: default
---


```{r setup, include=FALSE}

# Knitr global setup - change eval to true to run code

library(knitr)
knitr::opts_chunk$set(echo = TRUE, eval=FALSE, fig.show = "hold", fig.keep = "all")
opts_knit$set(root.dir = 'C:/Users/ap0y/Dropbox/Work Projects/PHD/Metabarcoding/HemipteraMetabarcodingMS')
setwd('C:/Users/ap0y/Dropbox/Work Projects/PHD/Metabarcoding/HemipteraMetabarcodingMS')
opts_chunk$set(dev = 'png')

#load scripts from scripts folder
read_chunk('scripts/summary_func.R')
```

```{r summary_func, echo=FALSE}
##Load utility functions from scripts folder (defined in markdown header)

read_chunk('scripts/summary_func.R')
```

# Introduction 

This is the R based reproducible workflow that performed the metabarcoding analyses presented for the manuscript " " by Batovska et al

The data that was analysed here includes 20 mock communities made up of Hemiptera, and 10 trap samples from a potato field. These were seperated over 3 runs on an illumina MiSeq

Run_1 - 15 pools of 100, 500, and 1000 insects
Run_2 - 5 pools of 250 insects, run in duplicate to compare combinatorial indexing and unique-dual indexing
Run_3 - 10 trap samples with varying numbers of insects

3 genes were multiplexed in the one PCR amplification, and libraries were prepared from these

In order to handle process hte , in this pipeline the samples are read in and processed within a for-loop, so only one sample is fully loaded into memory at a time. This keeps memory requirements quite low, and multiple MiSeq runs, or a Hiseq lane can be processed on 8GB of memory (although more is nice!).

## Reference database

dada2::assignTaxonomy expects a training fasta file in which the taxonomy corresponding to each sequence is encoded in the id line as below:

```{code}
>Level1;Level2;Level3;Level4;Level5;Level6;
ACCTAGAAAGTCGTAGATCGAAGTTGAAGCATCGCCCGATGATCGTCTGAAGCTGTAGCATGAGTCGATTTTCACATTCAGGGATACCATAGGATAC
```

For Species level assignment dada2::addSpecies expects a training fasta file  with the id line formatted as below:
```{code}
> ID Genus species
ACCTAGAAAGTCGTAGATCGAAGTTGAAGCATCGCCCGATGATCGTCTGAAGCTGTAGCATGAGTCGATTTTCACATTCAGGGATACCATAGGATAC
```
While the creators of DADA2 maintain formatted reference data for 16S and ITS regions, for the genes used in this study (COI, 18S, 12S) this database will need to be assembled and curated prior to beginning this workflow. This has been done within the associated Reference_builder.Rmd file



#Remove primers

DADA2 requires Non-biological nucleotides i.e. primers, adapters, linkers, etc to be removed. Prior to begining this workflow, samples were demultiplexed and illumina adapters were removed by the MiSeq software, however primer sequences still remain in the reads and must be removed prior to use with the DADA2 algorithm.

In this study there were 3 amplicons of different size generated in a multiplexed PCR. While COI should only contain the 5' primer, the 12S and 18S genes are length variable, and therefore may contain 3' primer sequences in addition to the 5'primer.

Therefore, for this workflow we will be using the Kmer based adapter trimming software BBDuk (Part of BBTools package https://jgi.doe.gov/data-and-tools/bbtools/) to trim the primers from our raw data files.

## Primers:            
```{code}
Name                    Illumina overhang adapter           Primer sequences
Sterno18S_F2_tail		ACACTCTTTCCCTACACGACGCTCTTCCGATCT		ATGCATGTCTCAGTGCAAG 
Sterno18S_R1_tail		GACTGGAGTTCAGACGTGTGCTCTTCCGATC 		TCGACAGTTGATAAGGCAGAC
Sterno12S_F2_tail		ACACTCTTTCCCTACACGACGCTCTTCCGATCT		CAYCTTGACYTAACAT
Sterno12S_R2_tail		GACTGGAGTTCAGACGTGTGCTCTTCCGATC			TAAAYYAGGATTAGATACCC
SternoCOI_F1_tail		ACACTCTTTCCCTACACGACGCTCTTCCGATCT		ATTGGWGGWTTYGGAAAYTG
SternoCOI_R1_tail		GACTGGAGTTCAGACGTGTGCTCTTCCGATC  		TATRAARTTRATWGCTCCTA
```

This script was handled externally in the linux terminal

```{bash bbduk trimming}
mkdir cleaned
ls | grep "R1_001.fastq.gz" | sort > test_ls_F
ls | grep "R2_001.fastq.gz" | sort > test_ls_R

let files=$(grep -c "fastq.gz" test_ls_F)

declare -i x
x=1


while [ $x -le $files ] 
	do

queryF=$(sed -n "${x}p" test_ls_F)
queryR=$(sed -n "${x}p" test_ls_R)

sample_nameF=$(echo $queryF | awk -F . '{ print $1}')
sample_nameR=$(echo $queryR | awk -F . '{ print $1}')

#Need to set location of bbduk - on basc it is contained in my root ~/bbmap/bbduk.sh

#Trim 3' primers from forward and reverse reads and any bases to the left
~/bbmap/bbduk.sh in=$sample_nameF.fastq.gz in2=$sample_nameR.fastq.gz out=$sample_nameF.temp.fastq.gz out2=$sample_nameR.temp.fastq.gz literal=ATTGGWGGWTTYGGAAAYTG,TATRAARTTRATWGCTCCTA,ATGCATGTCTCAGTGCAAG,TCGACAGTTGATAAGGCAGAC,CAYCTTGACYTAACAT,TAAAYYAGGATTAGATACCC copyundefined k=14 ordered=t rcomp=f ktrim=l tbo tpe;

#Trim 5' primers from forward and reverse reads and any bases to the right
~/bbmap/bbduk.sh in=$sample_nameF.temp.fastq.gz in2=$sample_nameR.temp.fastq.gz out=./cleaned/$sample_nameF.trimmed.fastq.gz out2=./cleaned/$sample_nameR.trimmed.fastq.gz literal=GGGTATCTAATCCTRRTTTA,ATGTTARGTCAAGRTG copyundefined k=14 ordered=t rcomp=f ktrim=r tbo tpe;
let x=x+1

done 2> bbduk_primer_trimming_stats.txt
rm *.temp.*

```

    
# DADA2 portion of workflow

## Set up

Load all requried packages

```{r load packages, message=FALSE, eval = FALSE }
sapply(c("dada2", "phyloseq","ggplot2","ips", "DECIPHER", "data.table", "ggplot2", "tidyverse","Biostrings","ShortRead","scales", "psadd","ggpubr"), require, character.only = TRUE)

```


## Error visualisation
We start by visualizing the quality profiles of the forward read and reverse reads for each run:

```{r pre filter plot, eval = FALSE, cache= TRUE}
runs <- dir("data/", pattern="run")

for (i in seq(along=runs)){
path <- paste0("data/",runs[i]) # CHANGE ME to the directory containing your demultiplexed forward-read fastq files

fastqFs <- sort(list.files(path, pattern="R1_001.trimmed.fastq.gz", full.names = TRUE))
fastqRs <- sort(list.files(path, pattern="R2_001.trimmed.fastq.gz", full.names = TRUE))
print(plotQualityProfile(fastqFs[1:4]) + ggtitle(paste0(runs[i]," Forward Reads")))
print(plotQualityProfile(fastqRs[1:4]) + ggtitle(paste0(runs[i]," Reverse Reads")))
}
```


## Filter and trim

The forward reads for the hemiptera metabarcoding data are of good quality, while The reverse reads are of slightly worse worse quality at the end, which is common in Illumina sequencing. Informed by these profiles, we will use the Truncate quality function (TruncQ=2) to cut the reads at any point the Q score crashes below 2.

```{r filter and trim}
##Note - for filtering stage, these parameters may not be optimal for each run or testset, use the previous plotting step to inform this
runs <- dir("data/", pattern="run")
filtered_out <- list()

for (i in seq(along=runs)){
  path <- paste0("data/",runs[i]) # CHANGE ME to the directory containing your demultiplexed forward-read fastq files
  filtpath <- file.path(path, "filtered") # Filtered forward files go into the path/filtered/ subdirectory
  
  fastqFs <- sort(list.files(path, pattern="R1_001.trimmed.fastq.gz"))
  fastqRs <- sort(list.files(path, pattern="R2_001.trimmed.fastq.gz"))
  
  if(length(fastqFs) != length(fastqRs)) stop(paste0("Forward and reverse files for ",runs[i]," do not match."))
  
  filtered_out[[i]] <- (filterAndTrim(fwd=file.path(path, fastqFs), filt=file.path(filtpath, fastqFs),
                                      rev=file.path(path, fastqRs), filt.rev=file.path(filtpath, fastqRs),
                                      maxEE=c(2,5), truncQ=2, maxN = 2, minLen = 100,
                                      rm.phix=TRUE, compress=TRUE, verbose=TRUE))
}
print(filtered_out)
```


## Post filtering error plotting

sanity check to see the effects of the filter and trim step

```{r Post filter plot, eval = FALSE, cache= TRUE}
runs <- dir("data/", pattern="run")

##Post filtering plotting
for (i in seq(along=runs)){
  path <- paste0("data/",runs[i]) # CHANGE ME to the directory containing your demultiplexed forward-read fastq files
  filtpath <- file.path(path, "filtered") # Filtered forward files go into the path/filtered/ subdirectory
  
  
  filtFs <- sort(list.files(filtpath, pattern="R1_001.trimmed.fastq.gz", full.names = TRUE))
  filtRs <- sort(list.files(filtpath, pattern="R2_001.trimmed.fastq.gz", full.names = TRUE))
  print(plotQualityProfile(filtFs[1:4]) + ggtitle(paste0(runs[i]," Forward Reads")))
  print(plotQualityProfile(filtRs[1:4]) + ggtitle(paste0(runs[i]," Reverse Reads")))
}

```

## Infer sequence variants

Every amplicon dataset has a different set of error rates and the DADA2 algorithm makes use of a parametric error model (err) to model this and infer real biological sequence variation from error. Following error model learning, all identical sequencing reads are dereplicated into into “Exact sequence variants” with a corresponding abundance equal to the number of reads with that unique sequence. The forward and reverse reads are then merged together by aligning the denoised forward reads with the reverse-complement of the corresponding reverse reads, and then constructing the merged “contig” sequences. Following this step, a sequence variant table is constructed and saved as an RDS file.

For this analysis we will use all the reads to estimate error rate, and plot the error model for each run as a sanity check

```{r Learn error rates }
runs <- dir("data/", pattern="run")
set.seed(100)

for (i in seq(along=runs)){
  path <- paste0("data/",runs[i]) # CHANGE ME to the directory containing your demultiplexed forward-read fastq files
  filtpath <- file.path(path, "filtered") # Filtered forward files go into the path/filtered/ subdirectory
  
  filtFs <- list.files(filtpath, pattern="R1_001.trimmed.fastq.gz", full.names = TRUE)
  filtRs <- list.files(filtpath, pattern="R2_001.trimmed.fastq.gz", full.names = TRUE)
  
  sample.names <- sapply(strsplit(basename(filtFs), "_"), `[`, 1) # Assumes filename = flowcell_samplename_XXX.fastq.gz
  sample.namesR <- sapply(strsplit(basename(filtRs), "_"), `[`, 1) # Assumes filename = flowcell_samplename_XXX.fastq.gz
  if(!identical(sample.names, sample.namesR)) stop("Forward and reverse files from run1 do not match.")
  names(filtFs) <- sample.names
  names(filtRs) <- sample.names
  
  # Learn error rates from samples
  # nread tells the function how many reads to use in error learning, this can be increased for more accuracy at the expense of runtime
  
  errF <- learnErrors(filtFs, multithread=TRUE)
  errR <- learnErrors(filtRs, multithread=TRUE)
  ##Print error plots to see how well the algorithm modelled the errors in the different runs
  print(plotErrors(errF, nominalQ=TRUE)+ ggtitle(paste0(runs[i]," Forward Reads")))
  print(plotErrors(errR, nominalQ=TRUE)+ ggtitle(paste0(runs[i]," Reverse Reads")))
  
  
  #Error inference and merger of reads
  mergers <- vector("list", length(sample.names))
  names(mergers) <- sample.names
  for(sam in sample.names) {
    cat("Processing:", sam, "\n")
    derepF <- derepFastq(filtFs[[sam]])
    ddF <- dada(derepF, err=errF, multithread=TRUE)
    
    derepR <- derepFastq(filtRs[[sam]])
    ddR <- dada(derepR, err=errR, multithread=TRUE)
    merger <- mergePairs(ddF, derepF, ddR, derepR) #, maxMismatch = 3
    mergers[[sam]] <- merger
  }
  

# Construct sequence table
seqtab<- makeSequenceTable(mergers)
saveRDS(seqtab, paste0(path,"/seqtab.rds")) # CHANGE ME to where you want sequence table saved
}
```


## Merge Runs, Remove Chimeras

Now that the sequence tables are created for each run, they need to be merged into a larger table representing the entire study. Following this, chimeric sequences are identified and removed using removeBimeraDenovo, and any identical sequences with the only difference being length variation are collapsed using collapseNoMismatch.

```{r merge runs and remove chimeras}
runs <- dir("data/", pattern="run")
stlist <- vector()

for (i in seq(along=runs)){
  path <- paste0("data/",runs[i]) # CHANGE ME to the directory containing your demultiplexed forward-read fastq files
  seqs <- list.files(path, pattern="seqtab.rds", full.names = TRUE)
  
  assign(paste("st", i, sep = ""),readRDS(seqs))
  stlist <- append(stlist, paste("st", i, sep = ""), after=length(seqs))
}

st.all <- mergeSequenceTables(st1, st2, st3)

st.all <- collapseNoMismatch(st.all, minOverlap = 20, orderBy = "abundance",
                                    vec = TRUE, verbose = TRUE)
seqtab.nochim <- removeBimeraDenovo(st.all, method="consensus", multithread=TRUE, verbose=TRUE)

print(paste(sum(seqtab.nochim)/sum(st.all),"of the abundance remaining after chimera removal"))

saveRDS(seqtab.nochim, "output/rds/seqtab_final.rds") # CHANGE ME to where you want sequence table saved

```

## Assign taxonomy

At this point we will assign kingdom to genus taxonomy to the sequence reads using the RDP classifier natively implemented in DADA2, with a bootstrap confidence of 80. Following this, species level taxonomy was added using exact matching.


```{r assign taxonomy}

seqtab.nochim <- readRDS("data/seqtab_final.rds")

# Assign Kingdom:Genus taxonomy using RDP classifier
tax <- assignTaxonomy(seqtab.nochim, "reference/merged_arthropoda_rdp_genus.fa", multithread=TRUE, minBoot=80, outputBootstraps=FALSE)
colnames(tax) <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus")

##add species to taxtable using exact matching
tax_plus <- addSpecies(tax, "reference/merged_arthropoda_rdp_species.fa", allowMultiple=TRUE)

##Add SP. to species rank for those with only a genus rank assignmnet
for(col in seq(7,ncol(tax_plus))) { 
  propagate <- is.na(tax_plus[,col]) & !is.na(tax_plus[,col-1])
  tax_plus[propagate,col:ncol(tax_plus)] <-  "spp."
}

##join genus and species name in species rank column
sptrue <- !is.na(tax_plus[,7])
tax_plus[sptrue,7] <- paste(tax_plus[sptrue,6],tax_plus[sptrue,7], sep=" ")

#Check Output
taxa.print <- tax_plus # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)

# Write taxonomy table to disk
saveRDS(tax_plus, "output/rds/tax_boot80_final.rds") 

```


## Make phyloseq 

Following taxonomic assignment, both the sequence table and taxonomic table are passed to the Phyloseq R package for further community analysis and visualisation of data. This involves loading in the sample info csv to merge with the sequence and taxonomy tables

```{r create PS, eval = FALSE}
seqtab.nochim <- readRDS("output/rds/seqtab_final.rds")
tax_plus <- readRDS("output/rds/tax_boot80_final.rds") 

#Load sample information
## ---- samdat ----
samdf <- read.csv("sample_data/Sample_info.csv", header=TRUE)
samdf <- samdf[!duplicated(samdf$SampleID),] #Remove duplicate entries for reverse reads

rownames(samdf) <- samdf$SampleID
keep.cols <- c("collection_date", "biome", "target_gene", "feature",
"sample_name" ,"SampleID","experimental_factor")
samdf <- samdf[rownames(seqtab.nochim), keep.cols]

#Display samDF
head(samdf)

## ---- phyloseq ----
ps <- phyloseq(tax_table(tax_plus), sample_data(samdf),
               otu_table(seqtab.nochim, taxa_are_rows = FALSE))
##save phyloseq object
saveRDS(ps, "output/rds/ps_final.rds") 
```


## Output tables of results

```{r output table, eval = FALSE}
ps <- readRDS("output/rds/ps_final.rds")

dir.create("output/csv")

##Export raw csv
export <- psmelt(ps)
write.csv(export, file = "output/csv/rawdata.csv")

#Subset data to Athropoda only & Export CSV
ps1 = subset_taxa(ps, Phylum == "Arthropoda") 
export <- psmelt(ps1)
write.csv(export, file = "output/csv/raw_arthropoda.csv")

#Convert arthropod data to proportions and apply filter threshold
psFR <- transform_sample_counts(ps1, fun = filterfun)
psFR <- transform_sample_counts(ps1, fun = proportions)
##Export filtered data
export <- psmelt(psFR)
write.csv(export, file = "output/csv/allgene_arthropoda_filt.csv")

#Agglomerate all OTUS by species level taxonomy and export
rm_c1 <-  c("Pool-C1-250","Pool-C2-250","Pool-C3-250","Pool-C4-250","Pool-C5-250")
spp_level <- subset_samples(psFR, sample_names(psFR)!=rm_c1)
spp_level = tax_glom(spp_level, "Species", NArm = TRUE)
sppexport <- psmelt(spp_level)
write.csv(sppexport, file = "output/csv/allgene_sppglom_filt.csv")

#Agglomerate all OTUS by species level taxonomy, then create summary table and export
sum_filt <- summarize_taxa(psFR, "Species", "SampleID")
sum_filt <- spread(sum_filt, key="SampleID", value="totalRA")
write.csv(sum_filt, file = "output/csv/allgene_sppglom_filt_summarized.csv")

```

#Plotting 

# Add expected samples 

As part of the mock community analysis, we wish to determine taxonomic bias by looking at observed vs expected reads. To do this, we load dummy sequence, taxonomy, and sample data tables and create a seperate phyloseq object, which will later be merged

This loads a dummy sequence table, taxonomy table, and sample data table and merges it into the existing phyloseq object


```{r get expected , eval = FALSE}
#Load expected information

exp_seqtab <- as.matrix(read.csv("sample_data/expected/exp_seqtab.csv",row.names=1, header=TRUE))
exp_taxtab <- as.matrix(read.csv("sample_data/expected/exp_taxtab.csv",row.names=1, header=TRUE))
exp_samdf <- read.csv("sample_data/expected/exp_samdf.csv", header=TRUE)

keep.cols <- c("collection_date", "biome", "target_gene", "feature",
"sample_name" ,"SampleID","experimental_factor")
rownames(exp_samdf) <- exp_samdf$SampleID
exp_samdf <- exp_samdf[rownames(exp_seqtab), keep.cols]

## Make phyloseq and merge
ps_exp <- phyloseq(tax_table(exp_taxtab), sample_data(exp_samdf),
               otu_table(exp_seqtab, taxa_are_rows = FALSE))

```

# Plots for all genes merged


```{r Plot all genes, eval = FALSE}
dir.create("output/figs")

positions = c('Pool-01-100', 'Pool-02-100', 'Pool-03-100', 'Pool-04-100', 'Pool-05-100', 'Pool-06-500', 'Pool-07-500', 'Pool-08-500', 'Pool-09-500', 'Pool-10-500', 'Pool-11-1000', 'Pool-12-1000', 'Pool-13-1000', 'Pool-14-1000', 'Pool-15-1000', 'Pool-U1-250', 'Pool-U2-250', 'Pool-U3-250', 'Pool-U4-250', 'Pool-U5-250', 'Trap-01', 'Trap-02', 'Trap-03', 'Trap-04', 'Trap-05', 'Trap-06', 'Trap-07', 'Trap-08', 'Trap-09', 'Trap-10')

#Merge in expecteds 
pstemp <- merge_phyloseq(ps1, ps_exp)
pstemp <- transform_sample_counts(pstemp, fun = filterfun) #Apply filter threhsold


#Drop Kingdom column so we have 3 genes merged 
tax_table(pstemp) <- tax_table(pstemp)[,2:7]


#export <- psmelt(psFR)
#write.csv(export, file = "filtered.csv")

##Plot mock communties
psmock = subset_samples(pstemp, biome == "Laboratory")
psmock = filter_taxa(psmock, function(x) mean(x) > 0, TRUE)
rm_c1 <-  c("Pool-C1-250","Pool-C2-250","Pool-C3-250","Pool-C4-250","Pool-C5-250")
psmock <- subset_samples(psmock, sample_names(psmock)!=rm_c1)
rm_c1_exp<- c("Pool-C1-250-exp","Pool-C2-250-exp","Pool-C3-250-exp","Pool-C4-250-exp","Pool-C5-250-exp")
psmock <- subset_samples(psmock, sample_names(psmock)!=rm_c1_exp)
psmock = tax_glom(psmock, "Species", NArm = TRUE)
psmock <- transform_sample_counts(psmock, fun= proportions) # Reset scale to 1 following NArm

mdf <- psmelt(psmock)
p <- ggplot(mdf, aes_string(x= "SampleID", y="Abundance", fill= "Genus" ))
p = p + geom_bar(stat = "identity", position = "stack", color = "NA")  
p = p + theme(axis.text.x = element_text(angle = -90, hjust = 0))
p = p + ggtitle(paste0("Relative abundance of taxa for all genes in mock communities"))
p = p + xlab("Mock Community") 
p = p + scale_fill_brewer(palette="RdYlBu")
p = p + theme_pubclean()
p1 = p + facet_wrap(feature~sample_name, drop=TRUE, scales="free_x") + xlab("feature")+ theme(axis.text.x = element_blank())

psreal = subset_samples(pstemp, experimental_factor != "Exp")
psreal <- subset_samples(psreal, sample_names(psreal)!=rm_c1)

psglom = tax_glom(psreal, "Species", NArm = TRUE)
psglom <- transform_sample_counts(psglom, fun= proportions) # Reset scale to 1 following NArm
p2 <- plot_heatmap(psglom, "NMDS", "bray", taxa.label="Species", title= paste0("All genes Heatmap Species level"), taxa.order="Order", trans=log_trans(2)) + scale_x_discrete(limits = positions)


psglom2 = tax_glom(psreal, "Genus", NArm = TRUE)
psglom2 <- transform_sample_counts(psglom2, fun= proportions) # Reset scale to 1 following NArm
p3 <- plot_heatmap(psglom2, "NMDS", "bray", taxa.label="Genus", title= paste0("All genes Heatmap Genus level"), taxa.order="Order", trans=log_trans(2)) + scale_x_discrete(limits = positions)

##Presence absense for each gene

mdt = fast_melt(psFR)
# Add the variable indicated in `GroupBy`, if provided.
    sdt = data.table(SampleID = sample_names(psFR),
                     var1 = get_variable(psFR, "SampleID"))
    setnames(sdt, "var1", "SampleID")
    # Join
    setkey(sdt, SampleID)
    setkey(mdt, SampleID)
    mdt <- sdt[mdt]
  # Summarize
  summarydt = mdt[, list(totalRA = sum(RelativeAbundance)),
                  by = c("Kingdom","Order","Family", "Genus","Species", "SampleID", "taxaID")]
  summarydt <- summarydt[!is.na(summarydt$Species)]
  summarydt$totalRA[summarydt$totalRA > 0] <- 1
  
  summarydt$totalRA <- replace(summarydt$totalRA,summarydt > 0, 1)
  
  summarydt$test <- paste(summarydt$Order,summarydt$Family,summarydt$Species,sep="_")
  
p4 <- ggplot(summarydt, aes(Kingdom, Species)) + geom_tile(aes(fill = totalRA),
   colour = "White") + theme_pubclean() + theme(legend.position="none") + coord_fixed(ratio=.5) + theme(axis.text.x = element_text(angle = -90, hjust = 0)) 


pdf(file= "output/figs/allgenes_plots.pdf", paper="a4")
plot(p1)
plot(p2)
plot(p3)
plot(p4)
dev.off()
```



```{r presence absense for each gene, include=FALSE}


##Code for getting order for heatmaps
RadialTheta <- function(x) {
    
    x <- as(x, "matrix")
    theta <- atan2((x[, 2] - mean(x[, 2])), (x[, 1] - mean(x[, 1])))
    names(theta) <- rownames(x)
    
    theta
    
}

## ordination_object is the output of ordinate()
glom_ordinate <- ordinate(psglom2, "NMDS", "bray")
my_heatmap_order = order(RadialTheta(glom_ordinate$species))


 rankcol = which(rank_names(physeq) %in% taxa.order)
        taxmat = as(tax_table(physeq)[, 1:rankcol], "matrix")
        taxa.order = apply(taxmat, 1, paste, sep = "", collapse = "")
        names(taxa.order) <- taxa_names(physeq)
        taxa.order = names(sort(taxa.order, na.last = TRUE))

```


# Supplementary Figures - Plot each gene seperately

NEED TO CHECK, IS THIS CURRENTLY USING THE FILTERED DATA???

```{r Seperate genes}

positions = c('Pool-01-100', 'Pool-02-100', 'Pool-03-100', 'Pool-04-100', 'Pool-05-100', 'Pool-06-500', 'Pool-07-500', 'Pool-08-500', 'Pool-09-500', 'Pool-10-500', 'Pool-11-1000', 'Pool-12-1000', 'Pool-13-1000', 'Pool-14-1000', 'Pool-15-1000', 'Pool-U1-250', 'Pool-U2-250', 'Pool-U3-250', 'Pool-U4-250', 'Pool-U5-250', 'Trap-01', 'Trap-02', 'Trap-03', 'Trap-04', 'Trap-05', 'Trap-06', 'Trap-07', 'Trap-08', 'Trap-09', 'Trap-10')

genes <- unique(psmelt(ps) %>% select(Kingdom))
genes <- as.vector(genes$Kingdom)
genes <- genes[!is.na(genes)]

for (i in seq(along=genes)){
  ####PROBLEM HERE WILL BE THE MOCKS!
print(genes[i])
ps_gene = subset_taxa(ps, Kingdom == genes[i])
ps_gene = subset_taxa(ps_gene, Phylum == "Arthropoda")

ps_genexp <- merge_phyloseq(ps_gene, ps_exp)
tax_table(ps_genexp) <- tax_table(ps_genexp)[,2:7]

#Summary export
summary <-  subset_samples(ps_genexp, experimental_factor == "O")
sp_summary <- summarize_taxa(summary, "Species", "SampleID")
sp_summary <- spread(sp_summary, key="SampleID", value="totalRA")
write.csv(sp_summary, file = paste0("output/csv/",genes[i],"_sppglom_filt_summarized.csv"))

##Transform data to proportions and set low proportions to zero - NEEDED TO REMOVE INDEX SWITCHING

psra_gene <- transform_sample_counts(ps_genexp, fun = filterfun)
##Remove zero counts
psra_gene = filter_taxa(psra_gene, function(x) mean(x) > 0, TRUE) #Used to be 1e-6

##Figure  1 - Mock communities

##Plot mock communties
psmock_gene = subset_samples(psra_gene, biome == "Laboratory")
psmock_gene = filter_taxa(psmock_gene, function(x) mean(x) > 0, TRUE)
rm_c1 <-  c("Pool-C1-250","Pool-C2-250","Pool-C3-250","Pool-C4-250","Pool-C5-250")
psmock_gene <- subset_samples(psmock_gene, sample_names(psmock_gene)!=rm_c1)
rm_c1_exp<- c("Pool-C1-250-exp","Pool-C2-250-exp","Pool-C3-250-exp","Pool-C4-250-exp","Pool-C5-250-exp")
psmock_gene <- subset_samples(psmock_gene, sample_names(psmock_gene)!=rm_c1_exp)
psmock_gene = tax_glom(psmock_gene, "Species", NArm = TRUE)
psmock_gene <- transform_sample_counts(psmock_gene, fun= proportions) # Reset scale to 1 following NArm


mdf <- psmelt(psmock_gene)
p <- ggplot(mdf, aes_string(x= "SampleID", y="Abundance", fill= "Genus" ))
p = p + geom_bar(stat = "identity", position = "stack", color = "NA")  
p = p + theme(axis.text.x = element_text(angle = -90, hjust = 0))
p = p + ggtitle(paste0("Relative abundance of taxa for ", genes[i], " in mock communities"))
p = p + xlab("Mock Community") 
p = p + scale_fill_brewer(type="div", palette="RdYlBu")
p = p + theme_pubclean()
p1 = p + facet_wrap(feature~sample_name, drop=TRUE, scales="free_x") + xlab("feature")+ theme(axis.text.x = element_blank())

psreal_gene = subset_samples(psra_gene, experimental_factor != "Exp")
psreal_gene <- subset_samples(psreal_gene, sample_names(psreal_gene)!=rm_c1)

psglom = tax_glom(psreal_gene, "Species", NArm = TRUE)
p2 <- plot_heatmap(psglom, "NMDS", "bray", taxa.label="Species", title= paste0(genes[i]," Heatmap Species level"), taxa.order="Order", trans=log_trans(2)) + scale_x_discrete(limits = positions)

psglom2 = tax_glom(psreal_gene, "Genus", NArm = TRUE)
p3 <- plot_heatmap(psglom2, "NMDS", "bray", taxa.label="Genus", title= paste0(genes[i]," Heatmap Genus level"), taxa.order="Order", trans=log_trans(2)) + scale_x_discrete(limits = positions)

pdf(file= paste0("output/figs/",genes[i],"_plots.pdf"), paper="a4")
plot(p1)
plot(p2)
plot(p3)
dev.off()
}
```
