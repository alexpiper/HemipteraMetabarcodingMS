---
title: "Hemiptera metabarcoding analysis"
author: "Alexander Piper"
date: "2018/12/11"
output:
  html_document: default
  pdf_document: default
---


```{r setup, include=FALSE}

# Knitr global setup - change eval to true to run code

library(knitr)
knitr::opts_chunk$set(echo = TRUE, eval=FALSE, fig.show = "hold", fig.keep = "all")
opts_knit$set(root.dir = 'C:/Users/ap0y/Dropbox/Work Projects/PHD/Metabarcoding/HemipteraMetabarcodingMS')
setwd('C:/Users/ap0y/Dropbox/Work Projects/PHD/Metabarcoding/HemipteraMetabarcodingMS')
opts_chunk$set(dev = 'png')

#load scripts from scripts folder
read_chunk('scripts/summary_func.R')
```

```{r summary_func, echo=FALSE}
##Load utility functions from scripts folder (defined in markdown header)

read_chunk('scripts/summary_func.R')
```

# Introduction 

This is the R based reproducible workflow that performed the metabarcoding analyses presented for the manuscript " " by Batovska et al

The data that was analysed here includes 20 mock communities made up of Hemiptera, and 10 trap samples from a potato field. These were seperated over 3 runs on an illumina MiSeq

Run_1 - 15 pools of 100, 500, and 1000 insects
Run_2 - 5 pools of 250 insects, run in duplicate to compare combinatorial indexing and unique-dual indexing
Run_3 - 10 trap samples with varying numbers of insects

3 genes were multiplexed in the one PCR amplification, and libraries were prepared from these

In order to handle process hte , in this pipeline the samples are read in and processed within a for-loop, so only one sample is fully loaded into memory at a time. This keeps memory requirements quite low, and multiple MiSeq runs, or a Hiseq lane can be processed on 8GB of memory (although more is nice!).


## Set up

Load all requried packages

```{r load packages, message=FALSE, eval = FALSE }
sapply(c("dada2", "phyloseq","ggplot2","ips", "DECIPHER", "data.table", "ggplot2", "tidyverse","Biostrings","ShortRead","scales", "psadd","ggpubr","seqinr", "viridis"), require, character.only = TRUE)

```

#Remove primers

DADA2 requires Non-biological nucleotides i.e. primers, adapters, linkers, etc to be removed. Prior to begining this workflow, samples were demultiplexed and illumina adapters were removed by the MiSeq software, however primer sequences still remain in the reads and must be removed prior to use with the DADA2 algorithm.

In this study there were 3 amplicons of different size generated in a multiplexed PCR. While COI should only contain the 5' primer, the 12S and 18S genes are length variable, and therefore may contain 3' primer sequences in addition to the 5'primer.

Therefore, for this workflow we will be using the Kmer based adapter trimming software BBDuk (Part of BBTools package https://jgi.doe.gov/data-and-tools/bbtools/) to trim the primers from our raw data files.

## Primers:            
```{code}
Name                    Illumina overhang adapter           Primer sequences
Sterno18S_F2_tail		ACACTCTTTCCCTACACGACGCTCTTCCGATCT		ATGCATGTCTCAGTGCAAG 
Sterno18S_R1_tail		GACTGGAGTTCAGACGTGTGCTCTTCCGATC 		TCGACAGTTGATAAGGCAGAC
Sterno12S_F2_tail		ACACTCTTTCCCTACACGACGCTCTTCCGATCT		CAYCTTGACYTAACAT
Sterno12S_R2_tail		GACTGGAGTTCAGACGTGTGCTCTTCCGATC			TAAAYYAGGATTAGATACCC
SternoCOI_F1_tail		ACACTCTTTCCCTACACGACGCTCTTCCGATCT		ATTGGWGGWTTYGGAAAYTG
SternoCOI_R1_tail		GACTGGAGTTCAGACGTGTGCTCTTCCGATC  		TATRAARTTRATWGCTCCTA
```

This script was handled externally in the linux terminal

```{bash bbduk trimming}
mkdir cleaned
ls | grep "R1_001.fastq.gz" | sort > test_ls_F
ls | grep "R2_001.fastq.gz" | sort > test_ls_R

let files=$(grep -c "fastq.gz" test_ls_F)

declare -i x
x=1


while [ $x -le $files ] 
	do

queryF=$(sed -n "${x}p" test_ls_F)
queryR=$(sed -n "${x}p" test_ls_R)

sample_nameF=$(echo $queryF | awk -F . '{ print $1}')
sample_nameR=$(echo $queryR | awk -F . '{ print $1}')

#Need to set location of bbduk - on basc it is contained in my root ~/bbmap/bbduk.sh

#Trim 3' primers from forward and reverse reads and any bases to the left
~/bbmap/bbduk.sh in=$sample_nameF.fastq.gz in2=$sample_nameR.fastq.gz out=$sample_nameF.temp.fastq.gz out2=$sample_nameR.temp.fastq.gz literal=ATTGGWGGWTTYGGAAAYTG,TATRAARTTRATWGCTCCTA,ATGCATGTCTCAGTGCAAG,TCGACAGTTGATAAGGCAGAC,CAYCTTGACYTAACAT,TAAAYYAGGATTAGATACCC copyundefined k=14 ordered=t rcomp=f ktrim=l tbo tpe;

#Trim 5' primers from forward and reverse reads and any bases to the right
~/bbmap/bbduk.sh in=$sample_nameF.temp.fastq.gz in2=$sample_nameR.temp.fastq.gz out=./cleaned/$sample_nameF.trimmed.fastq.gz out2=./cleaned/$sample_nameR.trimmed.fastq.gz literal=GGGTATCTAATCCTRRTTTA,ATGTTARGTCAAGRTG copyundefined k=14 ordered=t rcomp=f ktrim=r tbo tpe;
let x=x+1

done 2> bbduk_primer_trimming_stats.txt
rm *.temp.*

```

    
# DADA2 portion of workflow



## Error visualisation
We start by visualizing the quality profiles of the forward read and reverse reads for each run:

```{r pre filter plot, eval = FALSE, cache= TRUE}
runs <- dir("data/", pattern="run")

for (i in seq(along=runs)){
path <- paste0("data/",runs[i]) # CHANGE ME to the directory containing your demultiplexed forward-read fastq files

fastqFs <- sort(list.files(path, pattern="R1_001.trimmed.fastq.gz", full.names = TRUE))
fastqRs <- sort(list.files(path, pattern="R2_001.trimmed.fastq.gz", full.names = TRUE))
print(plotQualityProfile(fastqFs[1:4]) + ggtitle(paste0(runs[i]," Forward Reads")))
print(plotQualityProfile(fastqRs[1:4]) + ggtitle(paste0(runs[i]," Reverse Reads")))
}
```


## Filter and trim

The forward reads for the hemiptera metabarcoding data are of good quality, while The reverse reads are of slightly worse worse quality at the end, which is common in Illumina sequencing. Informed by these profiles, we will use the Truncate quality function (TruncQ=2) to cut the reads at any point the Q score crashes below 2.

```{r filter and trim}
##Note - for filtering stage, these parameters may not be optimal for each run or testset, use the previous plotting step to inform this
runs <- dir("data/", pattern="run")
filtered_out <- list()

for (i in seq(along=runs)){
  path <- paste0("data/",runs[i]) # CHANGE ME to the directory containing your demultiplexed forward-read fastq files
  filtpath <- file.path(path, "filtered") # Filtered forward files go into the path/filtered/ subdirectory
  
  fastqFs <- sort(list.files(path, pattern="R1_001.trimmed.fastq.gz"))
  fastqRs <- sort(list.files(path, pattern="R2_001.trimmed.fastq.gz"))
  
  if(length(fastqFs) != length(fastqRs)) stop(paste0("Forward and reverse files for ",runs[i]," do not match."))
  
  filtered_out[[i]] <- (filterAndTrim(fwd=file.path(path, fastqFs), filt=file.path(filtpath, fastqFs),
                                      rev=file.path(path, fastqRs), filt.rev=file.path(filtpath, fastqRs),
                                      maxEE=c(2,5), truncQ=2, maxN = 2, minLen = 100,
                                      rm.phix=TRUE, compress=TRUE, verbose=TRUE))
}
print(filtered_out)
```


## Post filtering error plotting

sanity check to see the effects of the filter and trim step

```{r Post filter plot, eval = FALSE, cache= TRUE}
runs <- dir("data/", pattern="run")

##Post filtering plotting
for (i in seq(along=runs)){
  path <- paste0("data/",runs[i]) # CHANGE ME to the directory containing your demultiplexed forward-read fastq files
  filtpath <- file.path(path, "filtered") # Filtered forward files go into the path/filtered/ subdirectory
  
  
  filtFs <- sort(list.files(filtpath, pattern="R1_001.trimmed.fastq.gz", full.names = TRUE))
  filtRs <- sort(list.files(filtpath, pattern="R2_001.trimmed.fastq.gz", full.names = TRUE))
  print(plotQualityProfile(filtFs[1:4]) + ggtitle(paste0(runs[i]," Forward Reads")))
  print(plotQualityProfile(filtRs[1:4]) + ggtitle(paste0(runs[i]," Reverse Reads")))
}

```

## Infer sequence variants

Every amplicon dataset has a different set of error rates and the DADA2 algorithm makes use of a parametric error model (err) to model this and infer real biological sequence variation from error. Following error model learning, all identical sequencing reads are dereplicated into into “Exact sequence variants” with a corresponding abundance equal to the number of reads with that unique sequence. The forward and reverse reads are then merged together by aligning the denoised forward reads with the reverse-complement of the corresponding reverse reads, and then constructing the merged “contig” sequences. Following this step, a sequence variant table is constructed and saved as an RDS file.

For this analysis we will use all the reads to estimate error rate, and plot the error model for each run as a sanity check

```{r Learn error rates }
runs <- dir("data/", pattern="run")
set.seed(100)

for (i in seq(along=runs)){
  path <- paste0("data/",runs[i]) # CHANGE ME to the directory containing your demultiplexed forward-read fastq files
  filtpath <- file.path(path, "filtered") # Filtered forward files go into the path/filtered/ subdirectory
  
  filtFs <- list.files(filtpath, pattern="R1_001.trimmed.fastq.gz", full.names = TRUE)
  filtRs <- list.files(filtpath, pattern="R2_001.trimmed.fastq.gz", full.names = TRUE)
  
  sample.names <- sapply(strsplit(basename(filtFs), "_"), `[`, 1) # Assumes filename = flowcell_samplename_XXX.fastq.gz
  sample.namesR <- sapply(strsplit(basename(filtRs), "_"), `[`, 1) # Assumes filename = flowcell_samplename_XXX.fastq.gz
  if(!identical(sample.names, sample.namesR)) stop("Forward and reverse files from run1 do not match.")
  names(filtFs) <- sample.names
  names(filtRs) <- sample.names
  
  # Learn error rates from samples
  # nread tells the function how many reads to use in error learning, this can be increased for more accuracy at the expense of runtime
  
  errF <- learnErrors(filtFs, multithread=TRUE)
  errR <- learnErrors(filtRs, multithread=TRUE)
  ##Print error plots to see how well the algorithm modelled the errors in the different runs
  print(plotErrors(errF, nominalQ=TRUE)+ ggtitle(paste0(runs[i]," Forward Reads")))
  print(plotErrors(errR, nominalQ=TRUE)+ ggtitle(paste0(runs[i]," Reverse Reads")))
  
  
  #Error inference and merger of reads
  mergers <- vector("list", length(sample.names))
  names(mergers) <- sample.names
  for(sam in sample.names) {
    cat("Processing:", sam, "\n")
    derepF <- derepFastq(filtFs[[sam]])
    ddF <- dada(derepF, err=errF, multithread=TRUE)
    
    derepR <- derepFastq(filtRs[[sam]])
    ddR <- dada(derepR, err=errR, multithread=TRUE)
    merger <- mergePairs(ddF, derepF, ddR, derepR) #, maxMismatch = 3
    mergers[[sam]] <- merger
  }
  

# Construct sequence table
seqtab<- makeSequenceTable(mergers)
saveRDS(seqtab, paste0(path,"/seqtab.rds")) # CHANGE ME to where you want sequence table saved
}
```


## Merge Runs, Remove Chimeras

Now that the sequence tables are created for each run, they need to be merged into a larger table representing the entire study. Following this, chimeric sequences are identified and removed using removeBimeraDenovo, and any identical sequences with the only difference being length variation are collapsed using collapseNoMismatch.

```{r merge runs and remove chimeras}
runs <- dir("data/", pattern="run")
stlist <- vector()

for (i in seq(along=runs)){
  path <- paste0("data/",runs[i]) # CHANGE ME to the directory containing your demultiplexed forward-read fastq files
  seqs <- list.files(path, pattern="seqtab.rds", full.names = TRUE)
  
  assign(paste("st", i, sep = ""),readRDS(seqs))
  stlist <- append(stlist, paste("st", i, sep = ""), after=length(seqs))
}

st.all <- mergeSequenceTables(st1, st2, st3)

st.all <- collapseNoMismatch(st.all, minOverlap = 20, orderBy = "abundance",
                                    vec = TRUE, verbose = TRUE)
seqtab.nochim <- removeBimeraDenovo(st.all, method="consensus", multithread=TRUE, verbose=TRUE)

print(paste(sum(seqtab.nochim)/sum(st.all),"of the abundance remaining after chimera removal"))

saveRDS(seqtab.nochim, "output/rds/seqtab_final.rds") # CHANGE ME to where you want sequence table saved

```

### Dada2 assign tax

```{r}

seqtab.nochim <- readRDS("output/rds/seqtab_final.rds")

# Assign Kingdom:Genus taxonomy using RDP classifier
tax <- assignTaxonomy(seqtab.nochim, "reference/merging/merged_arthropoda_rdp_genus.fa", multithread=TRUE, minBoot=80, outputBootstraps=FALSE)
colnames(tax) <- c("Gene", "Phylum", "Class", "Order", "Family", "Genus")

##add species to taxtable using exact matching
tax_plus <- addSpecies(tax, "reference/merging/merged_arthropoda_rdp_species.fa", allowMultiple=TRUE)

##Add SP. to species rank for those with only a genus rank assignmnet
for(col in seq(7,ncol(tax_plus))) { 
  propagate <- is.na(tax_plus[,col]) & !is.na(tax_plus[,col-1])
  tax_plus[propagate,col:ncol(tax_plus)] <-  "spp."
}

##join genus and species name in species rank column
sptrue <- !is.na(tax_plus[,7])
tax_plus[sptrue,7] <- paste(tax_plus[sptrue,6],tax_plus[sptrue,7], sep=" ")

#Check Output
taxa.print <- tax_plus # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)

# Write taxonomy table to disk
saveRDS(tax_plus, "output/rds/tax_RDP_final.rds") 

```

## Assign taxonomy with IDTAXA


```{r IDTAXA}
library(DECIPHER); packageVersion("DECIPHER")

seqtab.nochim <- readRDS("output/rds/seqtab_final.rds")
trainingSet <- readRDS("reference/rdp_genus_rooted_idtaxa.rds")

dna <- DNAStringSet(getSequences(seqtab.nochim)) # Create a DNAStringSet from the ASVs

##Decide on threshold
ids <- IdTaxa(dna, trainingSet, processors=1,threshold = 60, verbose=TRUE) 

#plot(ids, trainingSet)


#delete existing file
cat("",file="idtaxa.csv")
for (i in 1:length(ids)){
 lines <- as.data.table(t(cbind(ids[[i]]$taxon,ids[[i]]$confidence)))
 rownames(lines) <- c("taxa","confidence")
write.table(lines,file="idtaxa.csv",sep=",",append=TRUE, col.names=FALSE)
}

ranks <-  c("Root","Kingdom", "Phylum","Class", "Order", "Family", "Genus") # ranks of interest
# Convert the output object of class "Taxa" to a matrix analogous to the output from assignTaxonomy
taxid <- t(sapply(ids, function(x) {
        taxa <- x$taxon
        taxa[startsWith(taxa, "unclassified_")] <- NA
        taxa
}))


#tax_add <- addSpecies(taxid, "reference/pruned_arthropoda_rdp_species.fa", allowMultiple = TRUE)


library(stringi)
taxid <- stri_list2matrix(lapply(taxid, unlist), byrow=TRUE, fill="NA")
taxid[startsWith(taxid, "NA")] <- NA

#Add sequences and column names to matrix
colnames(taxid) <- ranks; rownames(taxid) <- getSequences(seqtab.nochim)

#Subset to remove the root rank
taxid <- subset(taxid, select=c("Kingdom", "Phylum","Class", "Order", "Family", "Genus"))

#Name object the same as the output of addSpecies
tax_plus <- taxid

##Add unclassified to species rank for those with only a genus rank assignmnet
#for(col in seq(6,ncol(tax_plus))) {
#  propagate <- is.na(tax_plus[,col]) & !is.na(tax_plus[,col-1])
#  tax_plus[propagate,col:ncol(tax_plus)] <- paste(tax_plus[propagate,col-1], "unclassified", sep="_")
#}

##Add SP. to species rank for those with only a genus rank assignmnet
for(col in seq(7,ncol(tax_plus))) { 
  propagate <- is.na(tax_plus[,col]) & !is.na(tax_plus[,col-1])
  tax_plus[propagate,col:ncol(tax_plus)] <-  "spp."
}

##join genus and species name in species rank column
sptrue <- startsWith(tax_plus[,7], "spp.") %>% 
  tidyr::replace_na(FALSE)
tax_plus[sptrue,7] <- paste(tax_plus[sptrue,6],tax_plus[sptrue,7], sep="_")

#Check Output
taxa.print <- tax_plus # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)

# Write taxonomy table to disk
saveRDS(tax_plus, "output/rds/tax_IdTaxa_final.rds") 

```

## Make phyloseq 

Following taxonomic assignment, both the sequence table and taxonomic table are passed to the Phyloseq R package for further community analysis and visualisation of data. This involves loading in the sample info csv to merge with the sequence and taxonomy tables

```{r create PS, eval = FALSE}
seqtab.nochim <- readRDS("output/rds/seqtab_final.rds")
tax_plus <- readRDS("output/rds/tax_RDP_final.rds") 

#Load sample information
## ---- samdat ----
samdf <- read.csv("sample_data/Sample_info.csv", header=TRUE)
samdf <- samdf[!duplicated(samdf$SampleID),] #Remove duplicate entries for reverse reads

rownames(samdf) <- samdf$SampleID
keep.cols <- c("collection_date", "biome", "target_gene", "feature",
"pool_comp" ,"SampleID","experimental_factor")
samdf <- samdf[rownames(seqtab.nochim), keep.cols]

#Display samDF
head(samdf)

## ---- phyloseq ----
ps <- phyloseq(tax_table(tax_plus), sample_data(samdf),
               otu_table(seqtab.nochim, taxa_are_rows = FALSE))
##save phyloseq object
saveRDS(ps, "output/rds/ps_rdp.rds") 
```


## Output tables of results

```{r output table, eval = FALSE}
ps <- readRDS("output/rds/ps_rdp.rds")

dir.create("output/csv")
dir.create("output/csv/unfiltered/")
dir.create("output/csv/filtered/")
dir.create("output/csv/seperategenes/")

##Define helper functions

#Function to transform data to proportions
proportions = function(x){
  xprop = (x / sum(x))
  return(xprop)
}

##Function to transform data to proportions and set low proportions to zero
filterfun = function(x){
  xprop = (x / sum(x)) #Convert to proportions
  xprop[xprop < (1e-4)] <- 0 ## remove taxa under 0.0001, which is 0.01%
  return(xprop)
}


##Export raw csv
export <- psmelt(ps)
write.csv(export, file = "output/csv/unfiltered/rawdata.csv")

#Agglomerate all OTU's to Gene level and export proportions - For supplementary table
sum_gene <- transform_sample_counts(ps, fun = proportions)
sum_gene <- summarize_taxa(sum_gene, "Gene", "SampleID")
sum_gene <- spread(sum_gene, key="SampleID", value="totalRA")

write.csv(sum_gene, file = "output/csv/unfiltered/gene_summarized.csv")

#Subset data to Athropoda only & Export CSV
ps1 = subset_taxa(ps, Phylum == "Arthropoda") 
export <- psmelt(ps1)
write.csv(export, file = "output/csv/unfiltered/raw_arthropoda.csv")

#Agglomerate all OTUS and export summary table pre-filtering (for index switching figure)

sum_spp <- summarize_taxa(ps1, "Species", "SampleID")
sum_spp <- spread(sum_spp, key="SampleID", value="totalRA")
write.csv(sum_spp, file = "output/csv/unfiltered/allgene_sppglom_unfiltered_summarized.csv")

sum_gen <- summarize_taxa(ps1, "Genus", "SampleID")
sum_gen <- spread(sum_gen, key="SampleID", value="totalRA")
write.csv(sum_gen, file = "output/csv/unfiltered/allgene_genglom_unfiltered_summarized.csv")

#Convert arthropod data to proportions and apply filter threshold
psFR <- transform_sample_counts(ps1, fun = filterfun)
psFR <- transform_sample_counts(psFR, fun = proportions)
psFR = filter_taxa(psFR, function(x) mean(x) > 0, TRUE) #Drop missing

##Export filtered data
export <- psmelt(psFR)
write.csv(export, file = "output/csv/filtered/allgene_arthropoda_filt.csv")

#Agglomerate all filtered OTUS by species & Genus level taxonomy and export
rm_c1 <-  c("Pool-C1-250","Pool-C2-250","Pool-C3-250","Pool-C4-250","Pool-C5-250")
spp_level <- subset_samples(psFR, sample_names(psFR)!=rm_c1)
spp_level = tax_glom(spp_level, "Species", NArm = FALSE)
sppexport <- psmelt(spp_level)
write.csv(sppexport, file = "output/csv/filtered/allgene_sppglom_filt.csv")

gen_level <- subset_samples(psFR, sample_names(psFR)!=rm_c1)
gen_level = tax_glom(gen_level, "Genus", NArm = FALSE)
genexport <- psmelt(gen_level)
write.csv(genexport, file = "output/csv/filtered/allgene_genglom_filt.csv")

#Agglomerate all filtered OTUS by species level taxonomy, then create summary table and export
sum_spp_filt <- summarize_taxa(psFR, "Species", "SampleID")
sum_spp_filt <- spread(sum_spp_filt, key="SampleID", value="totalRA")
write.csv(sum_spp_filt, file = "output/csv/filtered/allgene_sppglom_filt_summarized.csv")

sum_gen_filt <- summarize_taxa(psFR, "Genus", "SampleID")
sum_gen_filt <- spread(sum_gen_filt, key="SampleID", value="totalRA")
write.csv(sum_gen_filt, file = "output/csv/filtered/allgene_genglom_filt_summarized.csv")

```

#Plotting 

# Add expected abundances for all samples 

As part of the mock community analysis, we wish to determine taxonomic bias by looking at observed vs expected reads. To do this, we load dummy sequence, taxonomy, and sample data tables and create a seperate phyloseq object, which will later be merged

This loads a dummy sequence table, taxonomy table, and sample data table and merges it into the existing phyloseq object

```{r get expected , eval = FALSE}
#Load expected information

exp_seqtab <- as.matrix(read.csv("sample_data/expected/exp_seqtab.csv",row.names=1, header=TRUE))
exp_taxtab <- as.matrix(read.csv("sample_data/expected/exp_taxtab.csv",row.names=1, header=TRUE))
exp_samdf <- read.csv("sample_data/expected/exp_samdf.csv", header=TRUE)

keep.cols <- c("collection_date", "biome", "target_gene", "feature",
"pool_comp" ,"SampleID","experimental_factor")
rownames(exp_samdf) <- exp_samdf$SampleID
exp_samdf <- exp_samdf[rownames(exp_seqtab), keep.cols]

## Make phyloseq and merge
ps_exp <- phyloseq(tax_table(exp_taxtab), sample_data(exp_samdf),
               otu_table(exp_seqtab, taxa_are_rows = FALSE))

```

## Plots for all genes


```{r Plot all genes, eval = FALSE}
dir.create("output/figs")

positions = c('Pool-01-100', 'Pool-02-100', 'Pool-03-100', 'Pool-04-100', 'Pool-05-100', 'Pool-06-500', 'Pool-07-500', 'Pool-08-500', 'Pool-09-500', 'Pool-10-500', 'Pool-11-1000', 'Pool-12-1000', 'Pool-13-1000', 'Pool-14-1000', 'Pool-15-1000', 'Pool-U1-250', 'Pool-U2-250', 'Pool-U3-250', 'Pool-U4-250', 'Pool-U5-250', 'Trap-01', 'Trap-02', 'Trap-03', 'Trap-04', 'Trap-05', 'Trap-06', 'Trap-07', 'Trap-08', 'Trap-09', 'Trap-10')

positions_exp = c('Pool-01-100','Pool-01-100-exp',  'Pool-02-100','Pool-02-100-exp', 'Pool-03-100','Pool-03-100-exp', 'Pool-04-100','Pool-04-100-exp', 'Pool-05-100','Pool-05-100-exp', 'Pool-U1-250','Pool-U1-250-exp', 'Pool-U2-250','Pool-U2-250-exp', 'Pool-U3-250','Pool-U3-250-exp', 'Pool-U4-250','Pool-U4-250-exp', 'Pool-U5-250','Pool-U5-250-exp', 'Pool-06-500','Pool-06-500-exp', 'Pool-07-500','Pool-07-500-exp', 'Pool-08-500','Pool-08-500-exp', 'Pool-09-500','Pool-09-500-exp', 'Pool-10-500','Pool-10-500-exp', 'Pool-11-1000','Pool-11-1000-exp', 'Pool-12-1000','Pool-12-1000-exp', 'Pool-13-1000','Pool-13-1000-exp', 'Pool-14-1000','Pool-14-1000-exp', 'Pool-15-1000','Pool-15-1000-exp')

#Figure 1 - Mock communities observed Vs Expected

#Merge in expecteds 
pstemp <- merge_phyloseq(psFR, ps_exp)
pstemp <- transform_sample_counts(pstemp, fun = proportions)
#pstemp = filter_taxa(pstemp, function(x) mean(x) > 0, TRUE) #Drop missing

#Drop Kingdom column so we have 3 genes merged 
tax_table(pstemp) <- tax_table(pstemp)[,2:7]


##Subset to mock communities
psmock = subset_samples(pstemp, biome == "Laboratory")
psmock = filter_taxa(psmock, function(x) mean(x) > 0, TRUE)
rm_c1 <-  c("Pool-C1-250","Pool-C2-250","Pool-C3-250","Pool-C4-250","Pool-C5-250")
psmock <- subset_samples(psmock, sample_names(psmock)!=rm_c1)
rm_c1_exp<- c("Pool-C1-250-exp","Pool-C2-250-exp","Pool-C3-250-exp","Pool-C4-250-exp","Pool-C5-250-exp")
psmock <- subset_samples(psmock, sample_names(psmock)!=rm_c1_exp)
psmock = tax_glom(psmock, "Species", NArm = TRUE)
psmock <- transform_sample_counts(psmock, fun= proportions) # Reset scale to 1 following NArm
mdf <- psmelt(psmock)


#Reorder to pool composition
mdf$SampleID <- factor(mdf$SampleID, levels = unique(mdf$SampleID[order(-mdf$pool_comp)]))

#Plot horizontal mock communities

p <- ggplot(mdf, aes(x= SampleID, y=Abundance,fill= Genus))
p = p + geom_bar(stat = "identity", position = "stack", color = "NA")  
p = p + theme(axis.text.x = element_text(angle = -90, hjust = 0))
p = p + ggtitle(paste0("Relative abundance of taxa for all genes in mock communities"))
p = p + xlab("Mock Community") 
#p = p + scale_fill_viridis(discrete = TRUE)
p = p + scale_fill_manual(values=c("#0c4687","#ae0707","#fa6e24","#3a9e82","#95cf77"))
p = p + theme_pubclean()
p1 =  p + scale_x_discrete(limits = positions_exp) + coord_flip()

#plot obs/exp seperated 2 cols
p2 = p + facet_wrap(.~experimental_factor, drop=TRUE, scales="free")  + coord_flip() #+ theme(axis.text.y = element_blank(), axis.ticks.y=element_blank())
#plot obs/exp seperated 2 rows
p3 = p + facet_wrap(~experimental_factor, nrow=2,ncol=1,drop=TRUE, scales="free") + coord_flip()

##Calculate observed vs expected
#pseudocode

psgen = tax_glom(psmock, "Genus", NArm = TRUE)
psgen <- transform_sample_counts(psgen, fun= proportions) # Reset scale to 1 following NArm
psgen <- psmelt(psgen)


mock_exp <- psgen %>%
  filter(experimental_factor=="Exp") %>%
  subset(select=c("Abundance","Sample","Genus"))
mock_exp$Sample <-str_remove(mock_exp$Sample,pattern="-exp")
colnames(mock_exp) <- c("Expected","Sample","Genus")

mock_obs <- psgen %>%
  filter(experimental_factor=="O") %>%
  subset(select=c("Abundance","Sample","Genus"))
colnames(mock_obs) <- c("Observed","Sample","Genus")

expobs <- left_join(mock_exp,mock_obs, by=c("Sample","Genus")) %>%
  filter(Observed>0)

g <- ggplot(expobs,aes(x=Expected,y=Observed)) + geom_point(aes(colour=Genus)) + geom_smooth(method="lm",se=FALSE,aes(color=Genus))
#+facet_grid(~Genus, scales="free")

#Get pearsons correlation 




#Figure 2 - All detecctions

#Drop Kingdom column so we have 3 genes merged 
psglom <- psFR
tax_table(psglom) <- tax_table(psglom)[,2:7]
psglom = tax_glom(psreal, "Species", NArm = TRUE)
psglom <- transform_sample_counts(psglom, fun= proportions) # Reset scale to 1 following NArm
psglom = filter_taxa(psglom, function(x) mean(x) > 0, TRUE)
p4 <- plot_heatmap(psglom, "NMDS", "bray", taxa.label="Species", taxa.order="Family") + theme_pubr()  + scale_x_discrete(limits = positions) + theme(axis.text.x = element_text(angle=60, hjust=1), axis.title.x=element_blank(), axis.title.y=element_blank(), legend.position="none") + scale_fill_viridis(trans=log_trans(10), na.value="grey10") 


#axis.text.y = element_blank()

##Presence absense for each gene 
 
  mdt = fast_melt(psFR)
  summarydt = mdt[, list(totalRA = sum(RelativeAbundance)),
                  by = c("Gene","Order","Family", "Genus","Species", "SampleID", "taxaID")]
  summarydt <- summarydt[!is.na(summarydt$Species)]
  summarydt$totalRA[summarydt$totalRA > 0] <- 1
  
  #Attempt rearragning  - Losing the duplicates Here!
  rankcol = which(rank_names(psglom) %in% "Family")
  taxmat = as(tax_table(psglom)[, 1:rankcol], "matrix")
  taxa.order = apply(taxmat, 1, paste, sep = "", collapse = "")
  names(taxa.order) <- taxa_names(psglom)
  taxa.order = as.tibble(names(sort(taxa.order, na.last = TRUE)))
  colnames(taxa.order) <- "taxaID"
  taxa.order$seq <- seq(1:nrow(taxa.order))
  reorder <- full_join(taxa.order,summarydt,by="taxaID") 

  reorder$Species <- factor(reorder$Species, levels = unique(reorder$Species[order(reorder$seq)]))
  
p5 <- ggplot(reorder, aes(Gene, Species)) + geom_point(aes(colour = Gene),size=3) + theme(legend.position="none") + coord_fixed(ratio=.5) + theme(axis.text.x = element_text(angle=60, hjust=1), axis.title.x=element_blank(), axis.title.y=element_blank()) + theme_void() + theme(legend.position="none")

pname <- ggplot(reorder, aes(x=c(1), Species) ) + geom_tile(aes(fill=reorder$Order))+ geom_text(label=reorder$Order) + theme_void() + theme(legend.position="none")


ggarrange(pname,p5,p4,nrow=1,ncol=3,widths = c(1,1,5),align="h")

#Save plots to file
pdf(file= "output/figs/allgenes_plots.pdf", paper="a4")
plot(p1)
plot(p2)
plot(p3)
plot(ggarrange(pname,p5,p4,nrow=1,ncol=3,widths = c(1,4,5),align="h"))
dev.off()


#Problem named taxa - Need to gsub names
#Rhopalosiphum insertum/oxyacanthae <- Rhopalosiphum insertum
#Rhopalosiphum rufiabdominale/rufiabdominalis <- Rhopalosiphum rufiabdominale
#Lonchoptera bifurcata/uniseta
#Lipaphis erysimi/pseudobrassicae


```


## Supplementary Figures - Plot each gene seperately

```{r Seperate genes}

positions = c('Pool-01-100', 'Pool-02-100', 'Pool-03-100', 'Pool-04-100', 'Pool-05-100', 'Pool-06-500', 'Pool-07-500', 'Pool-08-500', 'Pool-09-500', 'Pool-10-500', 'Pool-11-1000', 'Pool-12-1000', 'Pool-13-1000', 'Pool-14-1000', 'Pool-15-1000', 'Pool-U1-250', 'Pool-U2-250', 'Pool-U3-250', 'Pool-U4-250', 'Pool-U5-250', 'Trap-01', 'Trap-02', 'Trap-03', 'Trap-04', 'Trap-05', 'Trap-06', 'Trap-07', 'Trap-08', 'Trap-09', 'Trap-10')

genes <- unique(psmelt(ps) %>% select(Gene))
genes <- as.vector(genes$Gene)
genes <- genes[!is.na(genes)]

for (i in seq(along=genes)){
  ####PROBLEM HERE WILL BE THE MOCKS!
print(genes[i])
ps_gene = subset_taxa(psFR, Gene == genes[i])
ps_gene = subset_taxa(ps_gene, Phylum == "Arthropoda")

ps_genexp <- merge_phyloseq(ps_gene, ps_exp)
tax_table(ps_genexp) <- tax_table(ps_genexp)[,2:7]

#Summary export
summary <-  subset_samples(ps_genexp, experimental_factor == "O")
sp_summary <- summarize_taxa(summary, "Species", "SampleID")
sp_summary <- spread(sp_summary, key="SampleID", value="totalRA")
write.csv(sp_summary, file = paste0("output/csv/seperategenes/",genes[i],"_sppglom_filt_summarized.csv"))

gen_summary <- summarize_taxa(summary, "Genus", "SampleID")
gen_summary <- spread(gen_summary, key="SampleID", value="totalRA")
write.csv(gen_summary, file = paste0("output/csv/seperategenes/",genes[i],"_genglom_filt_summarized.csv"))

##Transform data to proportions and set low proportions to zero - NEEDED TO REMOVE INDEX SWITCHING

psra_gene <- transform_sample_counts(ps_genexp, fun = filterfun)
##Remove zero counts
psra_gene = filter_taxa(psra_gene, function(x) mean(x) > 0, TRUE) #Used to be 1e-6

##Subset to mock communities
psmock = subset_samples(psra_gene, biome == "Laboratory")
psmock = filter_taxa(psmock, function(x) mean(x) > 0, TRUE)
rm_c1 <-  c("Pool-C1-250","Pool-C2-250","Pool-C3-250","Pool-C4-250","Pool-C5-250")
psmock <- subset_samples(psmock, sample_names(psmock)!=rm_c1)
rm_c1_exp<- c("Pool-C1-250-exp","Pool-C2-250-exp","Pool-C3-250-exp","Pool-C4-250-exp","Pool-C5-250-exp")
psmock <- subset_samples(psmock, sample_names(psmock)!=rm_c1_exp)
psmock = tax_glom(psmock, "Species", NArm = TRUE)
psmock <- transform_sample_counts(psmock, fun= proportions) # Reset scale to 1 following NArm
mdf <- psmelt(psmock)


#Reorder to pool composition
mdf$SampleID <- factor(mdf$SampleID, levels = unique(mdf$SampleID[order(-mdf$pool_comp)]))

#Plot horizontal mock communities

p <- ggplot(mdf, aes(x= SampleID, y=Abundance,fill= Genus))
p = p + geom_bar(stat = "identity", position = "stack", color = "NA")  
p = p + theme(axis.text.x = element_text(angle = -90, hjust = 0))
p = p + ggtitle(paste0("Relative abundance of taxa for all genes in mock communities"))
p = p + xlab("Mock Community") 
p = p + scale_fill_manual(values=c("#0c4687","#ae0707","#fa6e24","#3a9e82","#95cf77"))
p = p + theme_pubclean() 
p1 =  p + facet_wrap(~experimental_factor, drop=TRUE, scales="free")  + coord_flip() #+ theme(axis.text.y = element_blank(), axis.ticks.y=element_blank())

#plot obs/exp seperated 2 rows
p2 = p + facet_wrap(~experimental_factor, nrow=2,ncol=1,drop=TRUE, scales="free") + coord_flip() 

psreal_gene = subset_samples(psra_gene, experimental_factor != "Exp")
psreal_gene <- subset_samples(psreal_gene, sample_names(psreal_gene)!=rm_c1)

psglom = tax_glom(psreal_gene, "Species", NArm = TRUE)
p3 <- plot_heatmap(psglom, "NMDS", "bray", taxa.label="Species", title= paste0(genes[i]," Heatmap Species level"), taxa.order="Family") + theme_pubr()  + scale_x_discrete(limits = positions) + theme(axis.text.x = element_text(angle=60, hjust=1), axis.title.x=element_blank(), axis.title.y=element_blank(), legend.position="none") + scale_fill_viridis(trans=log_trans(10), na.value="grey10") 

pdf(file= paste0("output/figs/",genes[i],"_plots.pdf"), paper="a4")
plot(p1)
plot(p2)
plot(p3)
dev.off()
}
```
